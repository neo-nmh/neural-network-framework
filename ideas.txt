main function psuedocode
{
    init data
    init network with layers and number of neurons per layer

    for i in range(epochs):
        network.forward
        network.backward

    test for accuracy on test data
    record stats
}


BACKPROPOGATION
learn vector calculus (not really)
calculate derivative of cost function with respect to outputs
feed this value back to prev layers and adjust the weights using differentiation chain rule

TO ADJUST 1 SINGLE WEIGHT CONNECTING TO LAST LAYER:
1. calculate derivative of loss with respect to activation of a single node
2. calculate derivative of activation of single node with respect to (weighted sum + bias) 
3. calculate derivative of (weighted sum + bias) with respect to 1 weight
4. weight = weight - (learningrate x gradient)

TO ADJUST 1 SINGLE WEIGHT MORE THAN 1 LAYER FROM LAST LAYER:
Do step 1, 2
3. calculate derivative of (weighted sum + bias) with respect to prev activation
4. repeat 2,3 until at layer of weight
5. calculate derivative of (weighted sum + bias) with respect to weight

bad design:
when creating the network, all layer sizes and layer input sizesmust be entered in manually and correctly


ideas:
chain rule vs error formulas for backprop


changes made:
added new activation function