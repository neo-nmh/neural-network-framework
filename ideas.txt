main function
{
    init data
    init network with n of layers and m of neurons per layer

    for i in range(epochs):
        network.forward
        for j in range(weights):
            network.backward

    test for accuracy on test data
}

bad design:
when creating the network, all layer sizes and layer input sizesmust be entered in manually and correctly

BACKPROPOGATION
learn vector calculus
calculate derivative of cost function with respect to outputs
feed this value back to prev layers and adjust the weights using differentiation chain rule

TO ADJUST 1 SINGLE WEIGHT CONNECTING TO LAST LAYER:
1. calculate derivative of loss with respect to activation of a single node
2. calculate derivative of activation of single node with respect to (weighted sum + bias) 
3. calculate derivative of (weighted sum + bias) with respect to 1 weight
4. weight = weight - (learningrate x gradient)

TO ADJUST 1 SINGLE WEIGHT MORE THAN 1 LAYER FROM LAST LAYER:
Do step 1, 2
3. calculate derivative of (weighted sum + bias) with respect to prev activation
4. repeat 2,3 until at layer of weight
5. calculate derivative of (weighted sum + bias) with respect to weight


Questions:


changes made:
loss function
changed feedForward method name to calculateActivation, less confusing
changed pseudocode of main function
tried to implement backpropogation, but will change approach since using the derived formulas is much more efficient