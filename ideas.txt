main function
{
    init data
    init network with n of layers and m of neurons per layer

    # batch or stochastic ??
    for i in range(epochs):
        network.feedforward
        network.backprop

    test for accuracy on test data
}

class activationFunction:
    def forward(self, inputs):

    def backward(self, inputs):

bad design:
when creating the network, all layer sizes and layer input sizesmust be entered in manually and correctly, can live with it though

BACKPROPOGATION
learn vector calculus
calculate derivative of cost function with respect to outputs
feed this value back to prev layers and adjust the weights using differentiation chain rule

TO ADJUST 1 SINGLE WEIGHT CONNECTING TO LAST LAYER:
1. calculate derivative of loss with respect to activation of a single node
2. calculate derivative of activation of single node with respect to (weighted sum + bias) 
3. calculate derivative of (weighted sum + bias) with respect to 1 weight
4. weight = weight - (learningrate x gradient)

TO ADJUST 1 SINGLE WEIGHT MORE THAN 1 LAYER FROM LAST LAYER:
Do step 1, 2
3. calculate derivative of (weighted sum + bias) with respect to prev activation
4. repeat 2,3 until at layer of weight
5. calculate derivative of (weighted sum + bias) with respect to weight

Node class might have array of gradients for each weight

changes made:
fixed potential bugs
added weighted sum attribute to node class
modified feedforward, changed to allow stochastic gradient descent

